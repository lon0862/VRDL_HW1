{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"efficientnet_b6_pretrain.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPZLmjELzkJ6OU4I6sXfiKD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"vRVgC5MKMX1g"},"source":["from google.colab import drive\n","import pandas as pd \n","import torch\n","import torchvision\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as T\n","import numpy as np\n","from PIL import Image\n","!pip install efficientnet_pytorch\n","from efficientnet_pytorch import EfficientNet\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hG2jf8xPLtUM"},"source":["drive.mount('/content/gdrive')  # link google drive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kjka9_5OL1UO"},"source":["label_header = [\"photo\", \"type\"]\n","labels = pd.read_csv(\"/content/gdrive/My Drive/VRDL/HW1/training_labels.txt\",\n","                     names=label_header, sep=' ')\n","train_fold = \"/content/gdrive/MyDrive/VRDL/HW1/train_images_new\"\n","valid_fold = \"/content/gdrive/MyDrive/VRDL/HW1/validation_images\"\n","\n","class_header = [\"type\"]\n","class_data = pd.read_csv(\"/content/gdrive/My Drive/VRDL/HW1/classes.txt\",\n","                         names=class_header)\n","test_fold = \"/content/gdrive/MyDrive/VRDL/HW1/test_images\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5KOsR2ZsL3eG"},"source":["imagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","\n","# adjust to bigger,random get 350*350,\n","# do other change to get more data,then normalize\n","train_transforms = T.Compose([\n","    T.Resize((375, 375)),\n","    T.RandomCrop((350, 350)),\n","    T.RandomHorizontalFlip(),\n","    T.RandomRotation(10),\n","    T.ToTensor(),\n","    T.Normalize(*imagenet_stats, inplace=True),\n","    T.RandomErasing(inplace=True)\n","])\n","\n","valid_transforms = T.Compose([\n","    T.Resize((350, 350)),\n","    T.ToTensor(),\n","    T.Normalize(*imagenet_stats)\n","])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oaelxdsMCeW"},"source":["# get train/valid dataset\n","train_dataset = ImageFolder(train_fold, train_transforms)\n","valid_dataset = ImageFolder(valid_fold, valid_transforms)\n","# get model\n","model = EfficientNet.from_pretrained('efficientnet-b6')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3e3OiQT8MGnJ"},"source":["# to see model's fc layer\n","model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yb07MC4DMLMR"},"source":["model.fc = torch.nn.Linear(2304, 200)\n","\n","# use gpu\n","model = model.cuda()\n","try:\n","    model.load_state_dict(torch.load(\n","        '/content/gdrive/MyDrive/VRDL/HW1/weight_eff.pth'))\n","    model.eval()\n","    print('have previous weight')\n","except:\n","    print('Cannot load previous weight')\n","\n","loss_fn = nn.CrossEntropyLoss()\n","opt = optim.Adam(model.parameters(), 1e-4)\n","scheduler = lr_scheduler.ReduceLROnPlateau(\n","            opt, mode='max', factor=0.1, patience=1,\n","            verbose=True, threshold=0.0001, threshold_mode='rel',\n","            cooldown=0, min_lr=0, eps=1e-08)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cPqs7KdGMN48"},"source":["torch.cuda.empty_cache()\n","# Create train/valid loaders\n","batch_size = 8\n","train_dataloader = DataLoader(train_dataset, batch_size,\n","                              shuffle=True, num_workers=2, pin_memory=True)\n","valid_dataloader = DataLoader(valid_dataset, batch_size,\n","                              shuffle=False, num_workers=2, pin_memory=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CTeSGyLhMP4q"},"source":["valid_loss_min = np.Inf\n","n = 15\n","\n","for epoch in range(n):\n","    # keep track of training and validation loss\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","    print('running epoch: {}'.format(epoch))\n","\n","    # train the model\n","    model.train()\n","    train_acc = 0.0\n","    valid_acc = 0.0\n","    for data, label in train_dataloader:\n","        torch.cuda.empty_cache()\n","        # move tensors to GPU if CUDA is available\n","        data = data.cuda()\n","        label = label.cuda()\n","        # clear the gradients of all optimized variables\n","        opt.zero_grad()\n","        # forward pass: compute predicted outputs\n","        # by passing inputs to the model\n","        pred = model(data)\n","        pred_label = torch.argmax(pred, dim=1)\n","\n","        # calculate the batch loss\n","        loss = loss_fn(pred, label)\n","        # backward pass: compute gradient of the loss\n","        # with respect to model parameters\n","        loss.backward()\n","        # update training loss\n","        train_loss += loss.item()*data.size(0)\n","        train_acc += np.sum(pred_label.cpu().numpy() == label.cpu().numpy())\n","        # change learning rate to loss\n","        scheduler.step(train_acc)\n","        # perform a single optimization step (parameter update)\n","        opt.step()\n","\n","    # validate the model\n","    model.eval()\n","    for data, label in valid_dataloader:\n","        data = data.cuda()\n","        label = label.cuda()\n","        opt.zero_grad()\n","        pred = model(data)\n","        pred_label = torch.argmax(pred, dim=1)\n","        loss = loss_fn(pred, label)\n","        loss.backward()\n","        valid_loss += loss.item()*data.size(0)\n","        valid_acc += np.sum(pred_label.cpu().numpy() == label.cpu().numpy())\n","        opt.step()\n","\n","    # calculate average losses\n","    train_loss = train_loss/len(train_dataloader.dataset)\n","    valid_loss = valid_loss/len(valid_dataloader.dataset)\n","    train_acc = train_acc/len(train_dataloader.dataset)\n","    valid_acc = valid_acc/len(valid_dataloader.dataset)\n","\n","    # print training/validation statistics\n","    print(f'\\tTraining Loss: {train_loss:.6f}')\n","    print(f'\\tValidation Loss: {valid_loss:.6f}')\n","    print(f'\\t Training acc: {train_acc:.6f}')\n","    print(f'\\tValidation acc: {valid_acc:.6f}')\n","\n","    if valid_loss <= valid_loss_min:\n","        torch.save(model.state_dict(),\n","                   '/content/gdrive/MyDrive/VRDL/HW1/weight_eff.pth')\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).Saving model ...'\n","              .format(valid_loss_min, valid_loss))\n"],"execution_count":null,"outputs":[]}]}